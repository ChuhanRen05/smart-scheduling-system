import os
import pandas as pd
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.monitor import Monitor
from env_schedule import ScheduleEnv

# ========= STEP 1: åŠ è½½æ•°æ® ==========
df = pd.read_csv("/Users/nicolesong/smart-scheduling-system/RL/calendar_activity_dataset_1000_rows.csv")
unique_activities = df["Activity Name"].unique().tolist()
activity_map = {name: i for i, name in enumerate(unique_activities)}
df["Activity Code"] = df["Activity Name"].map(activity_map)

# ========= STEP 2: ç¯å¢ƒè®¾ç½® ==========
base_env = ScheduleEnv(df, activity_map)
env = Monitor(base_env)  # åŠ ä¸Š Monitorï¼Œç”¨äºè®°å½• reward æ—¥å¿—

# ========= STEP 3: åŠ è½½å·²æœ‰æ¨¡å‹æˆ–æ–°å»º ==========
if os.path.exists("ppo_schedule_model.zip"):
    print("ğŸ“‚ åŠ è½½å·²æœ‰æ¨¡å‹ ppo_schedule_model.zip")
    model = PPO("MlpPolicy", env, verbose=1, tensorboard_log="./logs/tb/")
else:
    print("ğŸ†• æ–°å»º PPO æ¨¡å‹")
    model = PPO("MlpPolicy", env, verbose=1)

# ========= STEP 4: è®¾ç½®è¯„ä¼°å›è°ƒ ==========
eval_callback = EvalCallback(
    env,
    best_model_save_path="./logs/",
    log_path="./logs/",
    eval_freq=5000,  # æ¯ 5000 æ­¥è¯„ä¼°ä¸€æ¬¡
    deterministic=True,
    render=False
)

# ========= STEP 5: è®­ç»ƒæ¨¡å‹ ==========
model.learn(total_timesteps=100000, callback=eval_callback, tb_log_name="schedule_run_1")
model.save("ppo_schedule_model")
print("âœ… æ¨¡å‹å·²ä¿å­˜ä¸º ppo_schedule_model.zip")

# ========= STEP 6: ä½¿ç”¨è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ ==========
obs, _ = env.reset()
done = False
while not done:
    action, _ = model.predict(obs)
    obs, reward, terminated, truncated, _ = env.step(action)
    done = terminated or truncated

# ========= STEP 7: æ‰“å°æ¨èæ—¥ç¨‹ ==========
print("\nğŸ“… æ¨èæ—¥ç¨‹ï¼š")
for hour, act in enumerate(obs):
    act_name = base_env.inverse_activity_map[int(act)]
    print(f"{hour:02d}:00 - {act_name}")

print("\nğŸ§  å¥åº·æŒ‡æ•°å¾—åˆ†ï¼š", round(reward, 2))

